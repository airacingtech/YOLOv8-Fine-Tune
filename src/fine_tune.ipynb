{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ultralytics\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import shutil\n",
    "import tqdm\n",
    "\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "# os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
    "\n",
    "# Change these parameters to fit your needs\n",
    "EPOCHS = 10\n",
    "IMG_SIZE = 640 # YOLOv8 default is 640\n",
    "\n",
    "# Whether or not to use data augmentations\n",
    "DATA_AUGMENTATIONS = True\n",
    "\n",
    "# Dictionary to weighted dataset\n",
    "DATASET_WEIGHTS = {\n",
    "    'vimba_left': 10,\n",
    "    'vimba_right': 10,\n",
    "}\n",
    "\n",
    "# Whether or not to use hyperparameter tuning\n",
    "HYPERPARAMETER_TUNING = False\n",
    "# Whether or not to use ray tune for hyperparameter sweep / tuning\n",
    "USE_RAY_TUNE = False\n",
    "# Number of iterations for hyperparameter sweep / tuning\n",
    "TUNE_ITERS = 5\n",
    "\n",
    "CURR_DIR = os.getcwd()\n",
    "WORKSPACE_DIR = os.path.dirname(CURR_DIR)\n",
    "DATASETS_DIR = WORKSPACE_DIR + '/datasets/cvat_exported/'\n",
    "DATA_YAML = WORKSPACE_DIR + '/data.yaml'\n",
    "\n",
    "# Percetnage of dataset to use for training\n",
    "TRAIN_PERCENTAGE = 1.0\n",
    "\n",
    "# Whether or not to keep empty frames (frames with no labels) in the dataset\n",
    "KEEP_EMPTY_FRAMES = True\n",
    "# Percentage of empty frames to keep in the dataset if KEEP_EMPTY_FRAMES is True (randomly sampled)\n",
    "PERCENTAGE_EMPTY_FRAMES_TO_KEEP = 0.2\n",
    "\n",
    "# Flag to resume training from a previous checkpoint (false if training from scratch)\n",
    "RESUME_TRAINING = False\n",
    "RESUME_TRAINING_PATH = WORKSPACE_DIR + 'runs/train/exp/weights/best.pt'\n",
    "\n",
    "# Size of YOLOv8 model\n",
    "MODEL_SIZE = 'n' # 'n' ,'s', 'm', 'l', 'x'\n",
    "\n",
    "# Path to trained model weights\n",
    "MODELS_PATH = WORKSPACE_DIR + 'models/'\n",
    "MODEL_PATH = MODELS_PATH + '/yolov8_fine_tune.pt'\n",
    "\n",
    "# This line prevents the Kernel from crashing when running model.train() which calls a plotting function\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "ONNX_BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check System Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CUDA Available: \" + str(torch.cuda.is_available()))\n",
    "print(\"Torch CUDA Version: \" + str(torch.version.cuda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to make sure CUDA is available and does not say \"None\"\n",
    "ultralytics.utils.checks.collect_system_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_empty_label(label_dst : os.PathLike) -> None:\n",
    "    '''\n",
    "    Generates an empty label file with the correct format to handle empty frames.\n",
    "    '''\n",
    "    with open(label_dst, 'w') as f:\n",
    "        f.write(\"\")\n",
    "\n",
    "def choose_dataset_weight(img_src : os.PathLike, dataset_weight : dict, weighted_frames : int) -> int:\n",
    "    '''\n",
    "    Returns a dataset weight to use dataset_weights dictionary. Uses the file path to determine the dataset.\n",
    "    '''\n",
    "    dataset_name = img_src.split(\"/\")[-3]\n",
    "    for dataset, weight in dataset_weight.items():\n",
    "        if dataset in dataset_name:\n",
    "            # print(\"Using dataset weight:\", weight , \"for dataset:\", dataset)\n",
    "            weighted_frames[dataset] = weighted_frames.get(dataset, 0) + (weight - 1)\n",
    "            return weight\n",
    "    return 1\n",
    "\n",
    "def copy_data_yaml(label_src : os.PathLike, img_src : os.PathLike, label_dst : os.PathLike, img_dst : os.PathLike, empty_frames_kept : int, weighted_frames : dict) -> None:\n",
    "    '''\n",
    "    Copies the images and labels from one directory to another. Also handles the case where the label file is empty (i.e. does not exist).\n",
    "    '''\n",
    "    if not os.path.exists(label_src):\n",
    "        # print(\"Empty label file detected:\", label_src)\n",
    "        if KEEP_EMPTY_FRAMES:\n",
    "            if random.random() < PERCENTAGE_EMPTY_FRAMES_TO_KEEP:\n",
    "                empty_frames_kept += 1\n",
    "                for i in range(choose_dataset_weight(img_src, DATASET_WEIGHTS, weighted_frames)):\n",
    "                    img_dst_with_weight = img_dst[:-4] + \"_\" + str(i) + \".jpg\"\n",
    "                    label_dst_with_weight = label_dst[:-4] + \"_\" + str(i) + \".txt\"\n",
    "                    shutil.copy(img_src, img_dst_with_weight)\n",
    "                    generate_empty_label(label_dst_with_weight)\n",
    "                    # normalize_image(img_dst)\n",
    "    else:\n",
    "        for i in range(choose_dataset_weight(img_src, DATASET_WEIGHTS, weighted_frames)):\n",
    "            img_dst_with_weight = img_dst[:-4] + \"_\" + str(i) + \".jpg\"\n",
    "            label_dst_with_weight = label_dst[:-4] + \"_\" + str(i) + \".txt\"\n",
    "            shutil.copy(img_src, img_dst_with_weight)\n",
    "            shutil.copy(label_src, label_dst_with_weight)\n",
    "            # normalize_image(img_dst)\n",
    "\n",
    "def normalize_image(img_path : os.PathLike) -> None:\n",
    "    '''\n",
    "    Normalizes the image to the correct format for YOLOv8.\n",
    "    '''\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (640, 640))\n",
    "    cv2.imwrite(img_path, img)\n",
    "\n",
    "def format_datasets(datasets_path : os.PathLike, data_yaml : os.PathLike) -> None:\n",
    "    \"\"\"\n",
    "    Takes in a directory of datasets where each dataset is in the format of a COCO dataset\n",
    "    and then formats the datasets into a single dataset that YOLOv8 can use for training\n",
    "    placed in the 'data/' directory. This function will delete the 'data/' directory and recreate\n",
    "    it. The data.yaml file must be created manually and will be copied over to the 'data/'\n",
    "    directory. The function will also split the data into training, validation, and test sets.\n",
    "        \n",
    "    Args:\n",
    "        datasets_path (os.PathLike): The path to the directory containing the datasets.\n",
    "        data_yaml (os.PathLike): The path to the data.yaml file specifying the dataset. This must be\n",
    "            created manually.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    assert os.path.exists(datasets_path), f\"The dataset path, {datasets_path}, does not exist.\"\n",
    "    assert os.path.exists(data_yaml), f\"The data.yaml file, {data_yaml}, does not exist.\"\n",
    "\n",
    "    print(\"Extracting sim images from:\", datasets_path, \"for training/validation data\")\n",
    "\n",
    "    # Get all images and labels from the datasets\n",
    "    datasets_labels = []\n",
    "    for dataset_path in os.listdir(datasets_path):\n",
    "        full_path = os.path.join(datasets_path, dataset_path)\n",
    "        for img_file in os.listdir(full_path + \"/images/\"):\n",
    "            datasets_labels.append([full_path + \"/images/\" + img_file, full_path + \"/labels/\" + img_file[:-4] + \".txt\", dataset_path])\n",
    "\n",
    "    # Sort images by filename\n",
    "    datasets_labels = sorted(datasets_labels, key = lambda x: x[0])\n",
    "\n",
    "    # Shuffle images deterministically with seed\n",
    "    random.seed(0)\n",
    "    random.shuffle(datasets_labels)\n",
    "\n",
    "    # Split 70% training, 20% validation, 10% test\n",
    "    training_data = datasets_labels[:len(datasets_labels) * 7 // 10]\n",
    "    valid_data = datasets_labels[len(datasets_labels) * 7 // 10 :len(datasets_labels) * 8 // 10]\n",
    "    test_data = datasets_labels[len(datasets_labels) * 8 // 10 :]\n",
    "\n",
    "\n",
    "    # Create the directories for the training, validation, and test data\n",
    "    if os.path.exists(\"../data/\"):\n",
    "        print(\"Deleting and recreating 'data/' folder...\")\n",
    "        shutil.rmtree(\"../data/\")\n",
    "    os.mkdir(\"../data/\")\n",
    "    os.mkdir(\"../data/train/\")\n",
    "    os.mkdir(\"../data/train/images/\")\n",
    "    os.mkdir(\"../data/train/labels/\")\n",
    "    os.mkdir(\"../data/valid/\")\n",
    "    os.mkdir(\"../data/valid/images/\")\n",
    "    os.mkdir(\"../data/valid/labels/\")\n",
    "    os.mkdir(\"../data/test/\")\n",
    "    os.mkdir(\"../data/test/images/\")\n",
    "    os.mkdir(\"../data/test/labels/\")\n",
    "\n",
    "    # Copy over images and labels to new directories\n",
    "    print(\"Copying images and labels to new directories...\")\n",
    "    print(\"Copying training data:\")\n",
    "\n",
    "    # Create a new image number to avoid overwriting images in the same chance they have the same name\n",
    "    new_image_uuid = 0\n",
    "    empty_frames_kept = 0\n",
    "    weighted_frames = {}\n",
    "    train_frames = 0\n",
    "    valid_frames = 0\n",
    "    test_frames = 0\n",
    "    for img_src, label_src, dataset_path in tqdm.tqdm(training_data):\n",
    "        if (random.random() < TRAIN_PERCENTAGE):\n",
    "            new_image_name = img_src.split(\"/\")[-1][:-4] + \"_\" + str(new_image_uuid) + \".jpg\"\n",
    "            new_label_name = label_src.split(\"/\")[-1][:-4] + \"_\" + str(new_image_uuid) + \".txt\"\n",
    "            img_dst = os.path.join(\"../data/train/images/\", dataset_path + \"_\" + new_image_name)\n",
    "            label_dst = os.path.join(\"../data/train/labels/\", dataset_path + \"_\" + new_label_name)\n",
    "            copy_data_yaml(label_src, img_src, label_dst, img_dst, empty_frames_kept, weighted_frames)\n",
    "            new_image_uuid += 1\n",
    "            train_frames += 1\n",
    "    for img_src, label_src, dataset_path in tqdm.tqdm(valid_data):\n",
    "        new_image_name = img_src.split(\"/\")[-1][:-4] + \"_\" + str(new_image_uuid) + \".jpg\"\n",
    "        new_label_name = label_src.split(\"/\")[-1][:-4] + \"_\" + str(new_image_uuid) + \".txt\"\n",
    "        img_dst = os.path.join(\"../data/valid/images/\", dataset_path + \"_\" + new_image_name)\n",
    "        label_dst = os.path.join(\"../data/valid/labels/\", dataset_path + \"_\" + new_label_name)\n",
    "        copy_data_yaml(label_src, img_src, label_dst, img_dst, empty_frames_kept, weighted_frames)\n",
    "        new_image_uuid += 1\n",
    "        valid_frames += 1\n",
    "    for img_src, label_src, dataset_path in tqdm.tqdm(test_data):\n",
    "        new_image_name = img_src.split(\"/\")[-1][:-4] + \"_\" + str(new_image_uuid) + \".jpg\"\n",
    "        new_label_name = label_src.split(\"/\")[-1][:-4] + \"_\" + str(new_image_uuid) + \".txt\"\n",
    "        img_dst = os.path.join(\"../data/test/images/\", dataset_path + \"_\" + new_image_name)\n",
    "        label_dst = os.path.join(\"../data/test/labels/\", dataset_path + \"_\" + new_label_name)\n",
    "        copy_data_yaml(label_src, img_src, label_dst, img_dst, empty_frames_kept, weighted_frames)\n",
    "        new_image_uuid += 1\n",
    "        test_frames += 1\n",
    "\n",
    "    # Copy over data.yaml file from root directory\n",
    "    shutil.copy(data_yaml, \"../data/\")\n",
    "    print(\"Copied over 'data.yaml' file\")\n",
    "    print(\"Number of empty frames kept: \", empty_frames_kept)\n",
    "\n",
    "    print(\"Number of training frames: \", train_frames)\n",
    "    print(\"Number of validation frames: \", valid_frames)\n",
    "    print(\"Number of test frames: \", test_frames)\n",
    "    print(\"Additional weighted frames created for each dataset: \", weighted_frames)\n",
    "    print(\"Total Training Frames: \", train_frames + sum(weighted_frames.values()))\n",
    "    print(\"Finished creating directories for YOLOv8 training pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_datasets(DATASETS_DIR, DATA_YAML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_model_size() -> str:\n",
    "    \"\"\"\n",
    "    Takes the global variable MODEL_SIZE and returns the corresponding string\n",
    "    to pass to the YOLO class and print the model parameter size.\n",
    "\n",
    "    Returns:\n",
    "        str: The model string to pass to the YOLO class.\n",
    "    \"\"\"\n",
    "    if MODEL_SIZE == 'n':\n",
    "        print(\"Using YOLOv8 Nano model\")\n",
    "        return 'yolov8n-seg.pt'\n",
    "    elif MODEL_SIZE == 's':\n",
    "        print(\"Using YOLOv8 Small model\")\n",
    "        return 'yolov8s-seg.pt'\n",
    "    elif MODEL_SIZE == 'm':\n",
    "        print(\"Using YOLOv8 Medium model\")\n",
    "        return 'yolov8m-seg.pt'\n",
    "    elif MODEL_SIZE == 'l':\n",
    "        print(\"Using YOLOv8 Large model\")\n",
    "        return 'yolov8l-seg.pt'\n",
    "    elif MODEL_SIZE == 'x':\n",
    "        print(\"Using YOLOv8 Extra Large model\")\n",
    "        return 'yolov8x-seg.pt'\n",
    "    \n",
    "# Load yolov8 nano segmentation model\n",
    "if RESUME_TRAINING:\n",
    "    # TODO: Change this to the path of the model you want to resume training from\n",
    "    model = YOLO(RESUME_TRAINING_PATH)\n",
    "# Load yolov8 nano segmentation model\n",
    "else:\n",
    "    model = YOLO(choose_model_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find dataset images\n",
    "data_dir = CURR_DIR + '/../data/'\n",
    "curr_data_yaml = data_dir + 'data.yaml'\n",
    "TEST_PATH = data_dir + '/test/images/'\n",
    "\n",
    "start_time = time.time()\n",
    "# By default, the model trains on a single GPU\n",
    "model.train(data=curr_data_yaml, epochs=EPOCHS)\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(\"Time to train: \", training_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs a hyperparameter sweep and selects the best hyperparameters\n",
    "if HYPERPARAMETER_TUNING:\n",
    "    model.tune(use_ray=USE_RAY_TUNE, iterations=TUNE_ITERS)\n",
    "else:\n",
    "    print(\"Skipping hyperparameter tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to save trained model inferencing on test set\n",
    "test_results_path = data_dir + '/test/annotation_results'\n",
    "\n",
    "# Make sure the test save path exists\n",
    "if not os.path.exists(test_results_path):\n",
    "    os.makedirs(test_results_path)\n",
    "\n",
    "# Inferencee fine-tuned model on test images and save results\n",
    "for file in os.listdir(TEST_PATH):\n",
    "    file_path = os.path.join(TEST_PATH, file)\n",
    "    output = model.predict(file_path)\n",
    "    save_path = os.path.join(test_results_path, file)\n",
    "    cv2.imwrite(save_path, output[0].plot())\n",
    "\n",
    "print(\"Inference on test set complete. Results saved to: \", test_results_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Weights and Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model as a .pt file\n",
    "# model.save(MODEL_PATH)\n",
    "\n",
    "# Export the model as an .onnx file\n",
    "model.export(save_dir=MODELS_PATH, format='onnx', batch=ONNX_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.export(save_dir=MODELS_PATH, format='onnx', batch=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.export(save_dir=MODELS_PATH, format='onnx', batch=6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
